{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03ae3171",
   "metadata": {},
   "source": [
    "# Building RAG Apps with LangChain (OpenAI, ChromaDB)\n",
    "\n",
    "This guide is designed to demonstrate how to build RAG applications using LangChain. It answers questions about the textbook \"An Introduction to Statistical Learning\" by taking content from this textbook to provide context-specific responses. Chroma is used as the vector database to store the embeddings. OpenAI is used both to perform the embeddings as well as the LLM which gives the responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb29dbb",
   "metadata": {},
   "source": [
    "## Setup\n",
    "#### Load the API key and relevant libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1ad3186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44eb2f6",
   "metadata": {},
   "source": [
    "**In this guide, we use GPT-3.5 as our LLM model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693de51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "845153ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(\"ISLR_First_Printing.pdf\")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488bbb4c",
   "metadata": {},
   "source": [
    "## Create Embeddings of PDF\n",
    "\n",
    "1. Using the RecursiveCharacterTextSplitter, split the document into chunks of size 1000 characters with a 200 character overlap between chunks and a start index added to each chunk. The chunks are split based on characters like [\"\\n\\n\", \"\\n\", \" \", \"\"].\n",
    "2. Store the chunks in a vector db, Chroma, after using the OPENAIEmbeddings model to embed the text into vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d236c99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "                    chunk_size=1000, chunk_overlap=200, add_start_index=True\n",
    "                    )\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ff017b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c0add6",
   "metadata": {},
   "source": [
    "## Question Answering from PDF\n",
    "\n",
    "1. Retrieve similar embeddings from the DB after embedding the prompt and compare the prompt embeddings to the embeddings of the chunk. Retrieve 10 most common embeddings based on cosine similarity.\n",
    "2. Answer questions on the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b7d8926b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type='similarity', search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a8fbc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b73f74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear regression is a simple approach for predicting a quantitative response by fitting a linear relationship between variables. It involves estimating the intercept and slope coefficients using least squares regression. The goal is to create a model that closely fits the available data points."
     ]
    }
   ],
   "source": [
    "for chunk in rag_chain.stream(\"What is Linear Regression?\"):\n",
    "    print(chunk, end=\"\", flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
